{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DhouhaC/MyLibrary/blob/resnet-resources/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vb0tmx85KMEN"
   },
   "source": [
    "<h1><center>Residual Networks</center></h1>\n",
    "<h2>Summary</h2>\n",
    "\n",
    "<p>Plain deep neural networks suffer from convergence problems with more layers in the network. RestNet has been introduced to solve this problem make it possible to achieve better results with deeper networks without increasing training error. This architecture has been proposed by <b>[1]</b> and won ILSVRC and COCO 2015 competitions.</p>\n",
    "\n",
    "<p>Basically, a deeper network should not produce a higher training error. Indeed, it should at least use identity functions in excess layers. But, in practice this is not the case. Thus, this suggests that deep layers are unable to detect identity functions. Actually, this hypothesis motivated Residual connections in deep networks.</p>\n",
    "\n",
    "<p>Residual network is a <b>stack</b> of residual building blocks. Each of them is composed of 2 or 3 plain layers and a residual layers merged with an element wise addition (Figure 1).</p>\n",
    "\n",
    "\n",
    "<img src=\"images\\building_block.png\" alt=\"building block\" title=\"Residual Learning: a building block\" width=\"300\" height=\"300\" />\n",
    "<p><center>Figure 1: Residual Learning: a building block.</center></p>\n",
    "\n",
    "<p>RestNet architectures are composed basically of two types of building blocks. The first one is a building block and the second is a bottelneck block (Figure 2).</p>\n",
    "\n",
    "<img src=\"images\\bottelneck_building.png\" alt=\"bottelneck_building\" title=\"A building block and bottelneck block\" width=\"550\" height=\"550\" />\n",
    "<p><center>Figure 2: Building block (left) and bottelneck block (right).</center></p>\n",
    "\n",
    "<p>The paper proposes five architectures that differ basically in layers number, filters number per layer and use of bottelneck block building (Figure 3). In (Figure 4), an illustration of 34 residual layer.</p>\n",
    "\n",
    "<img src=\"images\\five_architectures.png\" alt=\"five_architectures\" title=\"five_architectures\" width=\"700\" height=\"700\" />\n",
    "<p><center>Figure 3: Five architectures proposed.</center></p>\n",
    "\n",
    "<img src=\"images\\34_residual.png\" alt=\"34_residual\" title=\"34_residual\" width=\"200\" height=\"200\" />\n",
    "<p><center>Figure 4: 34 Residual Network.</center></p>\n",
    "\n",
    "<p><b>[1]</b> He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</p>\n",
    "\n",
    "<h2>Implementation</h2>\n",
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AR995oa1Jyuz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#kernel size\n",
    "k_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building Block</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, ch_in, ch_out, downsample):\n",
    "        '''\n",
    "        downsample = Boolean\n",
    "        '''\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.first_stride = 2 if downsample else 1\n",
    "        self.first = nn.Conv2d(ch_in, ch_out, k_size, stride=self.first_stride)\n",
    "        self.second = nn.Conv2d(ch_out, ch_out, k_size)\n",
    "        self.batch = nn.BatchNorm2d(ch_out)\n",
    "        self.downsample = downsample\n",
    "        self.down = nn.Conv2d(ch_in, ch_out, 1, stride=2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #First layer\n",
    "        x_res = x\n",
    "        x = self.batch(self.first(x))\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Second layer\n",
    "        x = self.batch(self.second(x))\n",
    "        \n",
    "        if self.downsample:\n",
    "            x_res = self.batch(self.down(x))\n",
    "        \n",
    "        x += x_res\n",
    "        x = F.relu(self.batch(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bottelneck Block</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottelneckBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, ch_midd, downsample):\n",
    "        '''\n",
    "        downsample = Boolean\n",
    "        '''\n",
    "        super(BottelneckBlock, self).__init__()\n",
    "        self.first_stride = 2 if downsample else 1\n",
    "        self.first = nn.Conv2d(ch_in, ch_midd, 1, stride=first_stride)\n",
    "        self.second = nn.Conv2d(ch_midd, ch_midd, k_size)\n",
    "        self.third = nn.Conv2d(ch_midd, ch_out, 1)\n",
    "        self.batch1 = nn.BatchNorm2d(ch_midd)\n",
    "        self.batch2 = nn.BatchNorm2d(ch_out)\n",
    "        self.downsample = downsample\n",
    "        self.down = nn.Conv2d(ch_in, ch_out, 1, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #First layer\n",
    "        x_res = x\n",
    "        x = self.batch1(self.first(x))\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Second layer\n",
    "        x = self.batch1(self.second(x))\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Third layer\n",
    "        x = self.batch2(self.third(x))\n",
    "        \n",
    "        if self.downsample:\n",
    "            x_res = self.batch2(self.down(x))\n",
    "        \n",
    "        x += x_res\n",
    "        x = F.relu(self.batch2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Block Sequence</h3>\n",
    "<p>This class is a sequence of blocks of same size. Only the first block is with downsample.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockSequence(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, num_block):\n",
    "        super(BlockSequence, self).__init__()\n",
    "        \n",
    "        self.seq_block = nn.Sequential(*[BuildingBlock(ch_in, ch_out, True) if i == 0 //\n",
    "                                         else BuildingBlock(ch_in, ch_out, False) for i in range(num_block)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.seq_block(x)\n",
    "        return x\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottelneckSequence(nn.Module):\n",
    "    def __init__(self, ch_in, ch_midd, ch_out, num_block):\n",
    "        super(BottelneckSequence, self).__init__()\n",
    "        \n",
    "        self.seq_block = nn.Sequential(*[BottelneckBlock(ch_in, ch_midd, ch_out, True) if i == 0 //\n",
    "                                         else BottelneckBlock(ch_in, ch_midd, ch_out, False) for i in range(num_block)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.seq_block(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ch_in, ch_size, num_block):\n",
    "        super(BlockSequence, self).__init__()\n",
    "        self.conv = nn.Conv2d(ch_in, ch_size, 7,stride=2)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        \n",
    "        #here goes stack of sequence block. This depends on the architecture\n",
    "\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(3)\n",
    "        self.fc = nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
